{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mouni\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f3c3e17d234383a44e1fa9dc225b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/59937 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658eec8f59f543e69f3d3e1289ef6727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6660 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict  # Import Dataset and DatasetDict classes\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = r\"C:\\Users\\mouni\\Downloads\\archive\\Arabic_Poetry_Dataset.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# Preprocess the 'poem_count' column\n",
    "df['poem_count'] = df['poem_count'].str.extract(r'(\\d+)')\n",
    "df = df.dropna(subset=['poem_count'])\n",
    "df['poem_count'] = df['poem_count'].astype(int)\n",
    "df = df[df['poem_count'] <= 30]\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/aragpt2-base\")\n",
    "\n",
    "# Set the pad_token to eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Step 1: Split the data into training and test sets\n",
    "train_df = df.sample(frac=0.9, random_state=42)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "# Step 2: Keep only the specified columns\n",
    "columns_to_keep = ['poem_text', 'poem_title', 'poem_count']\n",
    "train_df = train_df[columns_to_keep]\n",
    "test_df = test_df[columns_to_keep]\n",
    "\n",
    "# Step 3: Combine the columns into a single column to create a well-formed prompt\n",
    "def create_prompt(row):\n",
    "    return f\"القصيدة تحت عنوان {row['poem_title']}:\\n{row['poem_text']}\"\n",
    "\n",
    "train_df['prompt'] = train_df.apply(create_prompt, axis=1)\n",
    "test_df['prompt'] = test_df.apply(create_prompt, axis=1)\n",
    "\n",
    "# Step 4: Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['prompt'], truncation=True, max_length=200, padding='max_length')\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[['prompt']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['prompt']])\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Create a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': tokenized_train_dataset,\n",
    "    'test': tokenized_test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'القصيدة تحت عنوان قل لابن بوران إن كان ابن بوران:\\nقُلْ لابن بورانَ إن كان ابنَ بوران\\nفإنَّ شكّيَ فيه جُلُّ إيماني\\nيا باطلاً أوهمْتنيه مَخايلُه\\nبلا دليل ولا تثبيتِ بُرهانِ\\nما أنتَ إلا خيالٌ طاف طائفه\\nوما هجائيك إلا هجْرُ وسْنان\\nقد كنتُ أحسبه شيئاً فأهجوه\\nحتى أزاح يقيني فيه حسباني\\n', '__index_level_0__': 48430}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mouni\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa27cb16fd8b497d865001d32e3c1374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59936 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 15.035, 'grad_norm': 8.02468490600586, 'learning_rate': 4.958705953016551e-05, 'epoch': 0.07}\n",
      "{'loss': 11.8577, 'grad_norm': 8.912354469299316, 'learning_rate': 4.9169947944474107e-05, 'epoch': 0.13}\n",
      "{'loss': 11.0717, 'grad_norm': 7.924450397491455, 'learning_rate': 4.87528363587827e-05, 'epoch': 0.2}\n",
      "{'loss': 10.659, 'grad_norm': 7.131441593170166, 'learning_rate': 4.83357247730913e-05, 'epoch': 0.27}\n",
      "{'loss': 10.2303, 'grad_norm': 7.8812994956970215, 'learning_rate': 4.7918613187399895e-05, 'epoch': 0.33}\n",
      "{'loss': 10.0629, 'grad_norm': 7.4868903160095215, 'learning_rate': 4.750150160170849e-05, 'epoch': 0.4}\n",
      "{'loss': 10.0251, 'grad_norm': 6.540153980255127, 'learning_rate': 4.708439001601709e-05, 'epoch': 0.47}\n",
      "{'loss': 9.7642, 'grad_norm': 8.012529373168945, 'learning_rate': 4.666727843032568e-05, 'epoch': 0.53}\n",
      "{'loss': 9.8018, 'grad_norm': 6.098832607269287, 'learning_rate': 4.625016684463428e-05, 'epoch': 0.6}\n",
      "{'loss': 9.6032, 'grad_norm': 6.378545761108398, 'learning_rate': 4.5833055258942875e-05, 'epoch': 0.67}\n",
      "{'loss': 9.5126, 'grad_norm': 5.14635705947876, 'learning_rate': 4.541594367325147e-05, 'epoch': 0.73}\n",
      "{'loss': 9.3051, 'grad_norm': 5.685311794281006, 'learning_rate': 4.499883208756007e-05, 'epoch': 0.8}\n",
      "{'loss': 9.2946, 'grad_norm': 5.322017192840576, 'learning_rate': 4.458172050186866e-05, 'epoch': 0.87}\n",
      "{'loss': 9.2489, 'grad_norm': 7.723047733306885, 'learning_rate': 4.416460891617726e-05, 'epoch': 0.93}\n",
      "{'loss': 9.2427, 'grad_norm': 5.014260768890381, 'learning_rate': 4.3747497330485855e-05, 'epoch': 1.0}\n",
      "{'loss': 9.1316, 'grad_norm': 6.712012767791748, 'learning_rate': 4.333038574479445e-05, 'epoch': 1.07}\n",
      "{'loss': 8.9412, 'grad_norm': 5.658688068389893, 'learning_rate': 4.291327415910305e-05, 'epoch': 1.13}\n",
      "{'loss': 9.0222, 'grad_norm': 5.7339558601379395, 'learning_rate': 4.249616257341164e-05, 'epoch': 1.2}\n",
      "{'loss': 8.8454, 'grad_norm': 4.870687484741211, 'learning_rate': 4.207905098772024e-05, 'epoch': 1.27}\n",
      "{'loss': 8.8567, 'grad_norm': 6.3774871826171875, 'learning_rate': 4.1661939402028835e-05, 'epoch': 1.33}\n",
      "{'loss': 8.8446, 'grad_norm': 6.501870632171631, 'learning_rate': 4.124482781633743e-05, 'epoch': 1.4}\n",
      "{'loss': 8.8857, 'grad_norm': 7.015088081359863, 'learning_rate': 4.082771623064602e-05, 'epoch': 1.47}\n",
      "{'loss': 8.634, 'grad_norm': 6.353138446807861, 'learning_rate': 4.0410604644954616e-05, 'epoch': 1.53}\n",
      "{'loss': 8.7152, 'grad_norm': 5.37040376663208, 'learning_rate': 3.999349305926321e-05, 'epoch': 1.6}\n",
      "{'loss': 8.8129, 'grad_norm': 6.035794734954834, 'learning_rate': 3.957638147357181e-05, 'epoch': 1.67}\n",
      "{'loss': 8.6861, 'grad_norm': 6.938854694366455, 'learning_rate': 3.9159269887880405e-05, 'epoch': 1.74}\n",
      "{'loss': 8.7131, 'grad_norm': 4.996114730834961, 'learning_rate': 3.8742158302189e-05, 'epoch': 1.8}\n",
      "{'loss': 8.6793, 'grad_norm': 5.107234001159668, 'learning_rate': 3.8325046716497597e-05, 'epoch': 1.87}\n",
      "{'loss': 8.539, 'grad_norm': 7.100625038146973, 'learning_rate': 3.790793513080619e-05, 'epoch': 1.94}\n",
      "{'loss': 8.4676, 'grad_norm': 5.571714401245117, 'learning_rate': 3.749082354511479e-05, 'epoch': 2.0}\n",
      "{'loss': 8.3955, 'grad_norm': 4.865610122680664, 'learning_rate': 3.7073711959423385e-05, 'epoch': 2.07}\n",
      "{'loss': 8.3786, 'grad_norm': 5.549016952514648, 'learning_rate': 3.665660037373198e-05, 'epoch': 2.14}\n",
      "{'loss': 8.3129, 'grad_norm': 4.897411823272705, 'learning_rate': 3.624032301121196e-05, 'epoch': 2.2}\n",
      "{'loss': 8.3392, 'grad_norm': 6.498447895050049, 'learning_rate': 3.582404564869194e-05, 'epoch': 2.27}\n",
      "{'loss': 8.4248, 'grad_norm': 5.930882930755615, 'learning_rate': 3.5406934063000536e-05, 'epoch': 2.34}\n",
      "{'loss': 8.2287, 'grad_norm': 4.886629581451416, 'learning_rate': 3.498982247730913e-05, 'epoch': 2.4}\n",
      "{'loss': 8.3594, 'grad_norm': 6.101869583129883, 'learning_rate': 3.457271089161773e-05, 'epoch': 2.47}\n",
      "{'loss': 8.2736, 'grad_norm': 4.652952671051025, 'learning_rate': 3.4155599305926324e-05, 'epoch': 2.54}\n",
      "{'loss': 8.3359, 'grad_norm': 4.898025035858154, 'learning_rate': 3.373848772023492e-05, 'epoch': 2.6}\n",
      "{'loss': 8.2822, 'grad_norm': 4.493258953094482, 'learning_rate': 3.3321376134543517e-05, 'epoch': 2.67}\n",
      "{'loss': 8.2506, 'grad_norm': 5.060544490814209, 'learning_rate': 3.2905098772023496e-05, 'epoch': 2.74}\n",
      "{'loss': 8.3026, 'grad_norm': 6.508592128753662, 'learning_rate': 3.248798718633209e-05, 'epoch': 2.8}\n",
      "{'loss': 8.2651, 'grad_norm': 5.712185382843018, 'learning_rate': 3.207087560064069e-05, 'epoch': 2.87}\n",
      "{'loss': 8.2339, 'grad_norm': 5.106135368347168, 'learning_rate': 3.1653764014949284e-05, 'epoch': 2.94}\n",
      "{'loss': 8.1945, 'grad_norm': 5.8584818840026855, 'learning_rate': 3.123665242925788e-05, 'epoch': 3.0}\n",
      "{'loss': 8.0653, 'grad_norm': 5.928879261016846, 'learning_rate': 3.0819540843566476e-05, 'epoch': 3.07}\n",
      "{'loss': 7.9744, 'grad_norm': 4.6103034019470215, 'learning_rate': 3.040242925787507e-05, 'epoch': 3.14}\n",
      "{'loss': 8.0772, 'grad_norm': 6.159360885620117, 'learning_rate': 2.9985317672183665e-05, 'epoch': 3.2}\n",
      "{'loss': 8.058, 'grad_norm': 5.145822048187256, 'learning_rate': 2.956820608649226e-05, 'epoch': 3.27}\n",
      "{'loss': 8.0651, 'grad_norm': 4.836747646331787, 'learning_rate': 2.9151928723972238e-05, 'epoch': 3.34}\n",
      "{'loss': 8.0893, 'grad_norm': 5.390623092651367, 'learning_rate': 2.8734817138280834e-05, 'epoch': 3.4}\n",
      "{'loss': 8.0949, 'grad_norm': 6.534172534942627, 'learning_rate': 2.831770555258943e-05, 'epoch': 3.47}\n",
      "{'loss': 8.0225, 'grad_norm': 4.791749477386475, 'learning_rate': 2.7900593966898026e-05, 'epoch': 3.54}\n",
      "{'loss': 7.956, 'grad_norm': 4.732193946838379, 'learning_rate': 2.7484316604378002e-05, 'epoch': 3.6}\n",
      "{'loss': 8.0872, 'grad_norm': 8.977018356323242, 'learning_rate': 2.7067205018686598e-05, 'epoch': 3.67}\n",
      "{'loss': 7.9224, 'grad_norm': 5.88231897354126, 'learning_rate': 2.665092765616658e-05, 'epoch': 3.74}\n",
      "{'loss': 8.0182, 'grad_norm': 6.247636318206787, 'learning_rate': 2.6233816070475174e-05, 'epoch': 3.8}\n",
      "{'loss': 7.8399, 'grad_norm': 5.2390289306640625, 'learning_rate': 2.581670448478377e-05, 'epoch': 3.87}\n",
      "{'loss': 7.8541, 'grad_norm': 4.642305850982666, 'learning_rate': 2.5399592899092366e-05, 'epoch': 3.94}\n",
      "{'loss': 7.9135, 'grad_norm': 5.627585411071777, 'learning_rate': 2.4982481313400962e-05, 'epoch': 4.0}\n",
      "{'loss': 7.7804, 'grad_norm': 5.057235240936279, 'learning_rate': 2.4565369727709558e-05, 'epoch': 4.07}\n",
      "{'loss': 7.7059, 'grad_norm': 5.037236213684082, 'learning_rate': 2.4148258142018154e-05, 'epoch': 4.14}\n",
      "{'loss': 7.7746, 'grad_norm': 6.341775894165039, 'learning_rate': 2.373114655632675e-05, 'epoch': 4.2}\n",
      "{'loss': 7.8269, 'grad_norm': 6.120423793792725, 'learning_rate': 2.331486919380673e-05, 'epoch': 4.27}\n",
      "{'loss': 7.8106, 'grad_norm': 6.595069408416748, 'learning_rate': 2.2897757608115326e-05, 'epoch': 4.34}\n",
      "{'loss': 7.8217, 'grad_norm': 5.753904342651367, 'learning_rate': 2.2480646022423922e-05, 'epoch': 4.4}\n",
      "{'loss': 7.8463, 'grad_norm': 5.559892654418945, 'learning_rate': 2.2063534436732518e-05, 'epoch': 4.47}\n",
      "{'loss': 7.7801, 'grad_norm': 6.806793689727783, 'learning_rate': 2.1647257074212494e-05, 'epoch': 4.54}\n",
      "{'loss': 7.6815, 'grad_norm': 5.828802585601807, 'learning_rate': 2.123014548852109e-05, 'epoch': 4.6}\n",
      "{'loss': 7.8562, 'grad_norm': 6.452672958374023, 'learning_rate': 2.0813868126001067e-05, 'epoch': 4.67}\n",
      "{'loss': 7.6776, 'grad_norm': 6.381510257720947, 'learning_rate': 2.0396756540309663e-05, 'epoch': 4.74}\n",
      "{'loss': 7.8055, 'grad_norm': 4.710230350494385, 'learning_rate': 1.997964495461826e-05, 'epoch': 4.8}\n",
      "{'loss': 7.7999, 'grad_norm': 5.777447700500488, 'learning_rate': 1.9562533368926855e-05, 'epoch': 4.87}\n",
      "{'loss': 7.7672, 'grad_norm': 6.8197197914123535, 'learning_rate': 1.914542178323545e-05, 'epoch': 4.94}\n",
      "{'loss': 7.6263, 'grad_norm': 4.278034210205078, 'learning_rate': 1.8728310197544047e-05, 'epoch': 5.01}\n",
      "{'loss': 7.6013, 'grad_norm': 5.215024948120117, 'learning_rate': 1.8311198611852643e-05, 'epoch': 5.07}\n",
      "{'loss': 7.4786, 'grad_norm': 5.6714253425598145, 'learning_rate': 1.7894087026161243e-05, 'epoch': 5.14}\n",
      "{'loss': 7.5605, 'grad_norm': 5.201329231262207, 'learning_rate': 1.747780966364122e-05, 'epoch': 5.21}\n",
      "{'loss': 7.5125, 'grad_norm': 5.380942344665527, 'learning_rate': 1.706069807794981e-05, 'epoch': 5.27}\n",
      "{'loss': 7.5598, 'grad_norm': 7.286687850952148, 'learning_rate': 1.6643586492258408e-05, 'epoch': 5.34}\n",
      "{'loss': 7.6118, 'grad_norm': 5.127813339233398, 'learning_rate': 1.6226474906567004e-05, 'epoch': 5.41}\n",
      "{'loss': 7.6317, 'grad_norm': 5.977904796600342, 'learning_rate': 1.5810197544046983e-05, 'epoch': 5.47}\n",
      "{'loss': 7.6785, 'grad_norm': 5.333921909332275, 'learning_rate': 1.5393920181526963e-05, 'epoch': 5.54}\n",
      "{'loss': 7.8414, 'grad_norm': 7.172576427459717, 'learning_rate': 1.497680859583556e-05, 'epoch': 5.61}\n",
      "{'loss': 7.5953, 'grad_norm': 5.918280601501465, 'learning_rate': 1.4559697010144155e-05, 'epoch': 5.67}\n",
      "{'loss': 7.5885, 'grad_norm': 4.80149507522583, 'learning_rate': 1.4142585424452751e-05, 'epoch': 5.74}\n",
      "{'loss': 7.5309, 'grad_norm': 4.839820861816406, 'learning_rate': 1.3725473838761346e-05, 'epoch': 5.81}\n",
      "{'loss': 7.6432, 'grad_norm': 6.322969436645508, 'learning_rate': 1.3308362253069942e-05, 'epoch': 5.87}\n",
      "{'loss': 7.6021, 'grad_norm': 7.352390289306641, 'learning_rate': 1.2891250667378538e-05, 'epoch': 5.94}\n",
      "{'loss': 7.4982, 'grad_norm': 7.328291416168213, 'learning_rate': 1.2474139081687134e-05, 'epoch': 6.01}\n",
      "{'loss': 7.3734, 'grad_norm': 6.399250030517578, 'learning_rate': 1.2057861719167114e-05, 'epoch': 6.07}\n",
      "{'loss': 7.4882, 'grad_norm': 5.73879861831665, 'learning_rate': 1.1640750133475708e-05, 'epoch': 6.14}\n",
      "{'loss': 7.5424, 'grad_norm': 6.403966426849365, 'learning_rate': 1.1223638547784304e-05, 'epoch': 6.21}\n",
      "{'loss': 7.487, 'grad_norm': 5.145092010498047, 'learning_rate': 1.08065269620929e-05, 'epoch': 6.27}\n",
      "{'loss': 7.4505, 'grad_norm': 4.932000637054443, 'learning_rate': 1.0390249599572878e-05, 'epoch': 6.34}\n",
      "{'loss': 7.4423, 'grad_norm': 5.02580451965332, 'learning_rate': 9.973138013881474e-06, 'epoch': 6.41}\n",
      "{'loss': 7.5033, 'grad_norm': 6.473361015319824, 'learning_rate': 9.55602642819007e-06, 'epoch': 6.47}\n",
      "{'loss': 7.5221, 'grad_norm': 6.5612993240356445, 'learning_rate': 9.139749065670048e-06, 'epoch': 6.54}\n",
      "{'loss': 7.4481, 'grad_norm': 5.284121513366699, 'learning_rate': 8.722637479978644e-06, 'epoch': 6.61}\n",
      "{'loss': 7.4668, 'grad_norm': 5.000946044921875, 'learning_rate': 8.30552589428724e-06, 'epoch': 6.67}\n",
      "{'loss': 7.4162, 'grad_norm': 5.880112171173096, 'learning_rate': 7.888414308595836e-06, 'epoch': 6.74}\n",
      "{'loss': 7.4595, 'grad_norm': 5.488708019256592, 'learning_rate': 7.4713027229044316e-06, 'epoch': 6.81}\n",
      "{'loss': 7.4551, 'grad_norm': 4.962199687957764, 'learning_rate': 7.055025360384411e-06, 'epoch': 6.87}\n",
      "{'loss': 7.4633, 'grad_norm': 6.682959079742432, 'learning_rate': 6.6379137746930066e-06, 'epoch': 6.94}\n",
      "{'loss': 7.3363, 'grad_norm': 5.459978103637695, 'learning_rate': 6.220802189001602e-06, 'epoch': 7.01}\n",
      "{'loss': 7.3909, 'grad_norm': 5.8178791999816895, 'learning_rate': 5.803690603310198e-06, 'epoch': 7.07}\n",
      "{'loss': 7.3709, 'grad_norm': 4.661816120147705, 'learning_rate': 5.386579017618794e-06, 'epoch': 7.14}\n",
      "{'loss': 7.447, 'grad_norm': 5.470043182373047, 'learning_rate': 4.970301655098773e-06, 'epoch': 7.21}\n",
      "{'loss': 7.3602, 'grad_norm': 4.605388164520264, 'learning_rate': 4.553190069407368e-06, 'epoch': 7.27}\n",
      "{'loss': 7.3001, 'grad_norm': 6.915771007537842, 'learning_rate': 4.136078483715964e-06, 'epoch': 7.34}\n",
      "{'loss': 7.4184, 'grad_norm': 5.725259304046631, 'learning_rate': 3.7189668980245596e-06, 'epoch': 7.41}\n",
      "{'loss': 7.4017, 'grad_norm': 5.812681198120117, 'learning_rate': 3.301855312333155e-06, 'epoch': 7.47}\n",
      "{'loss': 7.4745, 'grad_norm': 5.506824016571045, 'learning_rate': 2.8847437266417517e-06, 'epoch': 7.54}\n",
      "{'loss': 7.3841, 'grad_norm': 5.799623489379883, 'learning_rate': 2.4676321409503473e-06, 'epoch': 7.61}\n",
      "{'loss': 7.3377, 'grad_norm': 6.630232334136963, 'learning_rate': 2.050520555258943e-06, 'epoch': 7.67}\n",
      "{'loss': 7.3607, 'grad_norm': 5.601804733276367, 'learning_rate': 1.6342431927389216e-06, 'epoch': 7.74}\n",
      "{'loss': 7.4022, 'grad_norm': 5.421024322509766, 'learning_rate': 1.2171316070475174e-06, 'epoch': 7.81}\n",
      "{'loss': 7.3439, 'grad_norm': 5.933392524719238, 'learning_rate': 8.000200213561131e-07, 'epoch': 7.87}\n",
      "{'loss': 7.3455, 'grad_norm': 5.968451499938965, 'learning_rate': 3.8290843566470907e-07, 'epoch': 7.94}\n",
      "{'train_runtime': 30190.6925, 'train_samples_per_second': 15.882, 'train_steps_per_second': 1.985, 'train_loss': 8.202004232574643, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=59936, training_loss=8.202004232574643, metrics={'train_runtime': 30190.6925, 'train_samples_per_second': 15.882, 'train_steps_per_second': 1.985, 'total_flos': 4.89392831232e+16, 'train_loss': 8.202004232574643, 'epoch': 7.999733057492742})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "# Initialize model\n",
    "model_name = \"aubmindlab/aragpt2-base\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Initialize Data Collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=8,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    evaluation_strategy=\"no\",  # Set evaluation strategy to \"no\"\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=2,\n",
    "    fp16=True,  # Enable mixed precision\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_dict['train'],\n",
    "    eval_dataset=dataset_dict['test'],\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./output/final_model\\\\tokenizer_config.json',\n",
       " './output/final_model\\\\special_tokens_map.json',\n",
       " './output/final_model\\\\vocab.json',\n",
       " './output/final_model\\\\merges.txt',\n",
       " './output/final_model\\\\added_tokens.json',\n",
       " './output/final_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model\n",
    "trainer.save_model(\"./output/final_model\")  # Save the final model to the specified path\n",
    "tokenizer.save_pretrained(\"./output/final_model\")  # Save tokenizer if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"./output/final_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806fc3cdba8945f2a6c4af8a58458fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 2.014402389526367, 'eval_runtime': 239.4681, 'eval_samples_per_second': 27.812, 'eval_steps_per_second': 3.479, 'epoch': 7.999733057492742}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from transformers import  DataCollatorForLanguageModeling\n",
    "\n",
    "# Initialize model\n",
    "model_name = \"aubmindlab/aragpt2-large\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_dict['train'],\n",
    "    eval_dataset=dataset_dict['test'],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: أكتب بيتًا شعريًا بأسلوب عربي فصيح يعبر عن ألم الخيانة والفراق بين الحبيبة وحبيبها، مستخدمًا كلمات مثل: خيانة، وجع، دمع، شوق، فراق، وداع. يجب أن يكون البيت مليئًا بالعاطفة ويعبر عن الحزن العميق.\n",
      "Generated: وإني لأعلم ما قد كنت أعلم أنني رجلُهوى وإننى ال يدى اللوى بها الدلى\n",
      "وأرى كل من رامى الشعر في الناس كلهم عدواَ ولا عدوةٍ له شرف الغنى\n",
      "فلا زلت ذا أدب لا يزال مع الأيام طالبال به العلم والتقى\n",
      "ولا زال هذا الدهر حتى صار آخر أيامه مقيماّةِ الحشا متهللاـعبةً\n",
      "على أنه كان لي قبل اليوم خير عيش وأوطار ومنزلْتضى العمر والعطب معاً\n",
      "وما أنا إلا العيش الذي لم يزل عندي غير أهله طامعي الهوى\n",
      "ولكن أرى نفسي بعد الموت أقرب منها إلى قرب منزلٍ بعيدتي\n",
      "وقد أصبحت فيما\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Function to generate text using the model with top-k and top-p sampling\n",
    "def generate_text(prompt, max_length=150, num_return_sequences=1, top_k=30, top_p=0.7):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=max_length + len(inputs['input_ids'][0]),  # Adjust max_length to account for the prompt length\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        do_sample=True,  # Enable sampling\n",
    "        top_k=top_k,     # Top-k sampling\n",
    "        top_p=top_p,     # Top-p (nucleus) sampling\n",
    "        temperature=0.2,  # Adjust temperature for more randomness\n",
    "        # pad_token_id=50256,\n",
    "        repetition_penalty=1.5,\n",
    "    )\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text[len(prompt):].strip()  # Remove the prompt from the generated text\n",
    "\n",
    "# Example prompts\n",
    "prompts = [\n",
    "     \"أكتب بيتًا شعريًا بأسلوب عربي فصيح يعبر عن ألم الخيانة والفراق بين الحبيبة وحبيبها، مستخدمًا كلمات مثل: خيانة، وجع، دمع، شوق، فراق، وداع. يجب أن يكون البيت مليئًا بالعاطفة ويعبر عن الحزن العميق.\"\n",
    "    #  \"أكتب قصيدة عن حب الوطن\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Generate and print text for each prompt\n",
    "for prompt in prompts:\n",
    "    generated_text = generate_text(prompt)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {generated_text}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
